# Projeto: Controle Or√ßament√°rio - De ponta a ponta (ETL, Data Quality e Analytics)

## üìå Vis√£o Geral
Este projeto √© focado em an√°lise de dados financeiros, mas com um diferencial: em vez de apenas conectar o Power BI em dados crus, eu constru√≠ um pipeline de **ETL com alicerces de Engenharia de Dados**. O objetivo √© garantir que qualquer an√°lise no Dashboard seja baseada em dados que j√° passaram por uma r√©gua rigorosa de qualidade e auditoria.

---

## üèóÔ∏è Arquitetura do Pipeline
Desenhei o projeto em camadas para separar bem as responsabilidades e garantir que o processo seja rastre√°vel:

1.  **Staging  (raw)**: Onde os dados aterrissam "como est√£o". √â aqui que identifico ru√≠dos, nulos e erros de preenchimento que gerei propositalmente via Python para simular um cen√°rio real.
2.  **Diagn√≥stico de Qualidade (Data Quality)**: Antes de carregar qualquer dado definitivo, rodo scripts de auditoria via SQL para validar se o dado est√° saud√°vel.
3.  **Trusted Layer (Dimens√µes e Fatos)**: √â a camada final. Aqui o dado j√° est√° limpo, tipado e com todas as chaves batendo. √â a √∫nica "fonte da verdade" do projeto.

---

## üõ†Ô∏è Tecnologias Utilizadas
* **SQL Server**: Onde acontece o "trabalho pesado" de limpeza, auditoria e modelagem.
* **Python**: Usei para gerar os dados sint√©ticos com regras de sazonalidade e erros controlados.
* **Power BI**: (Em constru√ß√£o) Camada para visualiza√ß√£o e KPIs de gest√£o.

---

## üìà Log de Desenvolvimento (Metodologia)

### [28/12/2025] Ingest√£o e Estrutura Inicial
* Configurei o ambiente SQL e a estrutura das primeiras tabelas.
* Fiz a carga de 5000+ registros via Bulk Insert na Staging.
* **Decis√£o t√©cnica:** Optei por usar **Views** para a transforma√ß√£o. Isso me permite testar toda a limpeza e as regras de neg√≥cio antes de dar o insert final na camada f√≠sica.

### [03/01/2026] Analytics Engineering: Auditoria e Carga das Dimens√µes
Nesta etapa, o foco foi garantir que as dimens√µes estivessem perfeitas. Sa√≠ da an√°lise "no olho" e implementei valida√ß√µes via c√≥digo:

* **Data Quality Autom√°tico:** Criei scripts para pegar espa√ßos extras e nulos/vazios de forma autom√°tica, garantindo que nenhum registro "sujo" passasse despercebido.
* **Resolu√ß√£o de Tipagem:** Identifiquei que os IDs vinham como decimais (`101.0`) e tratei isso com convers√µes aninhadas (`FLOAT -> INT`) direto na View.
* **Padroniza√ß√£o Inteligente (Initcap):** Desenvolvi uma l√≥gica de padroniza√ß√£o de nomes que respeita exce√ß√µes de neg√≥cio. O c√≥digo corrige o que est√° em caixa alta, mas preserva siglas (RH, TI) e termos t√©cnicos compostos.
* **Investiga√ß√£o de Causa Raiz:** Rastreando os dados, encontrei duplicidades ocultas por nulos (como no caso da categoria Aluguel) e saneei isso direto no pipeline.
* **Integridade de Chaves:** Validei as chaves estrangeiras entre as dimens√µes para garantir que nenhuma categoria ficasse √≥rf√£ de um centro de custo.

---

## üöÄ Pr√≥ximos Passos
- [ ] Aplicar esse mesmo rigor de Data Quality nas tabelas Fato.
- [ ] Validar a integridade referencial completa entre Fatos e Dimens√µes.
- [ ] Construir o Dashboard no Power BI com foco em Or√ßado vs. Realizado.

---

Este √© um projeto de portf√≥lio para demonstrar habilidades em ETL, BI e An√°lise de Dados.